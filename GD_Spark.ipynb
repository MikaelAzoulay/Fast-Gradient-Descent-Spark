{"nbformat":4,"nbformat_minor":0,"metadata":{"name":"GD_Project2_NB","notebookId":3487412734444862,"colab":{"name":"GD_Project2_NB.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"tDKuG5pRyvbd","colab_type":"code","colab":{},"outputId":"7de9aa7e-ce6d-46d5-fdf5-386766bd8cd0"},"source":["%scala\n","\n","import org.apache.spark.ml.feature.{Imputer, OneHotEncoderEstimator, StandardScaler, StringIndexer, VectorAssembler}\n","import org.apache.spark.ml.linalg.DenseVector\n","import org.apache.spark.rdd.RDD\n","import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n","import org.apache.spark.sql.functions.{lit, mean, rand, udf}\n","import org.apache.spark.sql.types._\n","import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row, SparkSession}\n","import org.apache.spark.{SparkConf, SparkContext}\n","\n","import scala.collection.mutable.ArrayBuffer\n","import scala.math.sqrt\n","import scala.util.Random.shuffle"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">import java.util.concurrent.{Callable, FutureTask, TimeUnit}\n","import org.apache.log4j.{Level, Logger}\n","import org.apache.spark.ml.feature.{Imputer, OneHotEncoderEstimator, StandardScaler, StringIndexer, VectorAssembler}\n","import org.apache.spark.ml.linalg.DenseVector\n","import org.apache.spark.rdd.RDD\n","import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n","import org.apache.spark.sql.functions.{lit, mean, rand, udf}\n","import org.apache.spark.sql.types._\n","import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row, SparkSession}\n","import org.apache.spark.{SparkConf, SparkContext}\n","import scala.collection.mutable.ArrayBuffer\n","import scala.math.sqrt\n","import scala.util.Random.shuffle\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"A71ecpfTyvbi","colab_type":"code","colab":{},"outputId":"13e11905-2479-4c73-f6cb-b16e85528a39"},"source":["%scala\n","\n","def getSS: SparkSession = {\n","    val spark = SparkSession\n","      .builder()\n","      .master(\"local\")\n","      .appName(\"Projet GD\")\n","      .config(\"spark.some.config.option\", \"some-value\")\n","      .getOrCreate()\n","    spark\n","  }\n","\n","case class GenericDSRow(labels: Double, features: Seq[Double])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">getSS: org.apache.spark.sql.SparkSession\n","defined class GenericDSRow\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"BnG3LKsiyvbm","colab_type":"code","colab":{},"outputId":"c44863ea-be47-4111-8810-5152b3fcdcd0"},"source":["%scala\n","\n","def grad(y: Double, w: Array[Double], x: Array[Double]): Array[Double] = {\n","  prod_by_scal(x, 2 * (prod_scal(w, x) - y))\n","}\n","\n","def subtr(x: Array[Double], y: Array[Double]) = {\n","  (x zip y).map(a => a._1 - (a._2))\n","}\n","\n","def prod_scal(x: Array[Double], y: Array[Double]): Double = (for ((a, b) <- x zip y) yield a * b).reduce((a, b) => a + b)\n","\n","def prod_by_scal(x: Array[Double], y: Double) = (for (z <- 0 to x.size - 1) yield (x(z)) * y).toArray\n","\n","def sum(x: Array[Double], y: Array[Double]) = (x zip y).map(a => a._1 + a._2)\n","\n","def mul(x: Array[Double], y: Array[Double]) = (x zip y).map(a => a._1 * a._2)\n","\n","def div(x: Array[Double], y: Array[Double]) = (x zip y).map(a => a._1 / a._2)\n","\n","def root(x: Array[Double]) = x.map(a => sqrt(a))\n","\n","def toArrayDeRow(miniers: Array[GenericDSRow]) = {\n","    var liste: ArrayBuffer[Row] = new ArrayBuffer[Row]()\n","    val schema = Encoders.product[GenericDSRow].schema\n","    //schema.printTreeString()\n","    for (m <- miniers) {\n","      val value = m.productIterator.toArray\n","      val row: Row = new GenericRowWithSchema(value, schema)\n","      liste += row\n","    }\n","    liste.toArray\n"," }\n","\n","def arrayOfRowToArray(l: Array[Row]) = {\n","    var liste: ArrayBuffer[(Double, Array[Double])] = new ArrayBuffer[(Double, Array[Double])]()\n","    for (el <- l) {\n","      liste += Tuple2(el.getDouble(0), el.getSeq(1).toArray) // getAs[Seq[Double]](1).toArray[Double])\n","    }\n","    liste.toArray\n","}"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">grad: (y: Double, w: Array[Double], x: Array[Double])Array[Double]\n","subtr: (x: Array[Double], y: Array[Double])Array[Double]\n","prod_scal: (x: Array[Double], y: Array[Double])Double\n","prod_by_scal: (x: Array[Double], y: Double)Array[Double]\n","sum: (x: Array[Double], y: Array[Double])Array[Double]\n","mul: (x: Array[Double], y: Array[Double])Array[Double]\n","div: (x: Array[Double], y: Array[Double])Array[Double]\n","root: (x: Array[Double])Array[Double]\n","toArrayDeRow: (miniers: Array[GenericDSRow])Array[org.apache.spark.sql.Row]\n","arrayOfRowToArray: (l: Array[org.apache.spark.sql.Row])Array[(Double, Array[Double])]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"E1uVv58Tyvbx","colab_type":"code","colab":{},"outputId":"a241d40e-0eb1-4b1e-ac34-40f8df41ddbb"},"source":["%scala\n","\n","def MBGD_local_SansSpark(W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[(Double, Array[Double])], tailleBatch: Int): Array[Double] = {\n","\n","    val N = r.length\n","    var W_local = W_b\n","    //println(\"wlocal=\",W_local)\n","    for (i <- 1 to epochsLocal) {\n","      var random_idx_list = shuffle(0 to N - 1)\n","      val batches_idx = random_idx_list.grouped(tailleBatch).toList\n","      for (batch_idx <- batches_idx) {\n","        //println(\"batch_idx\",batch_idx)\n","        val batch = batch_idx.map(index => r.apply(index))\n","        //println(\"batch\",batch)\n","        val temp = batch.map(x => grad(x._1, W_local, x._2)).reduce(sum(_, _))\n","        val nu_w = prod_by_scal(temp, nu / batch.length)\n","        W_local = subtr(W_local, nu_w)\n","      }\n","    }\n","    W_local\n","  }\n","\n","def MBGD_parallel_DF(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                       df: DataFrame, tailleBatch: Int): Array[Double] = {\n","    val spark = getSS\n","    var W = w\n","\n","    for (i <- 1 to epochs) {\n","      import spark.implicits._\n","      val W_b = sc.broadcast(W)\n","      val wLocauxDF = df.mapPartitions(\n","        iterator => { // Iterator[Row]\n","          var result = MBGD_local(W_b.value, nu, epochsLocal, iterator.toIterable.toArray[Row], tailleBatch)\n","          Seq(result).toIterator\n","        })\n","      W = wLocauxDF.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","        .map(_.toString.toDouble / wLocauxDF.rdd.getNumPartitions)).reduce(sum(_, _))\n","    } // Fin for\n","    W\n"," }\n","\n","def MBGD_local(W_b: Array[Double], nu: Double, epochsLocal: Int,\n","                 r: Array[Row], tailleBatch: Int): Array[Double] = {\n","    val N = r.length\n","    var W_local = W_b\n","\n","    for (i <- 1 to epochsLocal) {\n","      var random_idx_list = shuffle(0 to N - 1)\n","      val batches_idx = random_idx_list.grouped(tailleBatch).toList\n","      for (batch_idx <- batches_idx) {\n","        val batch = batch_idx.map(index => r.apply(index))\n","        //val temp = batch.map(x => grad(x.getDouble(0), W_local, x.getSeq(1).toArray[Double])).reduce(sum(_, _))\n","        val temp = batch.map(x => grad(x.getDouble(0), W_local, x.getSeq(1).toArray[Double])).reduce(sum(_, _))\n","        val nu_w = prod_by_scal(temp, nu / batch.length)\n","        W_local = subtr(W_local, nu_w)\n","      }\n","    }\n","    W_local\n","  }\n","\n","def MBGD_parallel_RDD(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, r: RDD[(Double, Array[Double])], tailleBatch: Int): Array[Double] = {\n","    val numPartitions = r.getNumPartitions\n","    val parts = r.glom()\n","    var W = w\n","    for (i <- 1 to epochs) {\n","      val W_b = sc.broadcast(W)\n","      val W_locaux = parts.map(x => MBGD_local_SansSpark(W_b.value, nu, epochsLocal, x, tailleBatch))\n","      W = W_locaux.map(w => w.map(_ / numPartitions)).reduce(sum(_, _))\n","    }\n","    W\n","  }\n","\n","def MBGD_parallel_DS(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                       ds: Dataset[GenericDSRow], tailleBatch: Int): Array[Double] = {\n","    val spark = getSS\n","    var W = w\n","\n","    for (i <- 1 to epochs) {\n","      import spark.implicits._\n","      val W_b = sc.broadcast(W)\n","      val wLocauxDF = ds.mapPartitions(\n","        iterator => { // Iterator[Row]\n","          val r2 = toArrayDeRow(iterator.toIterable.toArray[GenericDSRow])\n","          var result = MBGD_local(W_b.value, nu, epochsLocal, r2, tailleBatch)\n","          Seq(result).toIterator\n","        })\n","      W = wLocauxDF.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","        .map(_.toString.toDouble / wLocauxDF.rdd.getNumPartitions)).reduce(sum(_, _))\n","    } // Fin for\n","    W\n","  }"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">MBGD_local_SansSpark: (W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[(Double, Array[Double])], tailleBatch: Int)Array[Double]\n","MBGD_parallel_DF: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, df: org.apache.spark.sql.DataFrame, tailleBatch: Int)Array[Double]\n","MBGD_local: (W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[org.apache.spark.sql.Row], tailleBatch: Int)Array[Double]\n","MBGD_parallel_RDD: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, r: org.apache.spark.rdd.RDD[(Double, Array[Double])], tailleBatch: Int)Array[Double]\n","MBGD_parallel_DS: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, ds: org.apache.spark.sql.Dataset[GenericDSRow], tailleBatch: Int)Array[Double]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"N4i3TMANyvb9","colab_type":"code","colab":{},"outputId":"332af66e-8633-49d7-fb93-b4a4a0c5a6bc"},"source":["%scala\n","\n","  def MOM_MBGD_local(W_b: Array[Double], nu: Double, epochsLocal: Int,\n","                     r: Array[(Double, Array[Double])], tailleBatch: Int, M_b: Array[Double],\n","                     beta: Double): (Array[Double], Array[Double]) = {\n","\n","    val N = r.length\n","    var W_local = W_b\n","    var M_local = M_b\n","    for (i <- 1 to epochsLocal) {\n","      var random_idx_list = shuffle(0 to N - 1)\n","      val batches_idx = random_idx_list.grouped(tailleBatch).toList\n","      for (batch_idx <- batches_idx) {\n","        val batch = batch_idx.map(index => r.apply(index))\n","        val temp = batch.map(x => grad(x._1, W_local, x._2)).reduce(sum(_, _))\n","        val nu_w = prod_by_scal(temp, nu / batch.length)\n","        val mbeta = prod_by_scal(M_local, beta)\n","        M_local = sum(mbeta, nu_w)\n","        W_local = subtr(W_local, M_local)\n","      }\n","    }\n","    (W_local, M_local)\n","  }\n","\n"," def MOM_MBGD_parallel_RDD(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                            r: RDD[(Double, Array[Double])], tailleBatch: Int, beta: Double): Array[Double] = {\n","    val numPartitions = r.getNumPartitions\n","    val parts = r.glom()\n","    var W = w\n","    var M = w\n","    for (i <- 1 to epochs) {\n","      val W_b = sc.broadcast(W)\n","      val M_b = sc.broadcast(M)\n","      val W_M_locaux = parts.map(x => MOM_MBGD_local(W_b.value, nu, epochsLocal, x, tailleBatch, M_b.value, beta))\n","      val W_locaux = W_M_locaux.map(x => x._1)\n","      val M_locaux = W_M_locaux.map(x => x._2)\n","      W = W_locaux.map(w => w.map(_ / numPartitions)).reduce(sum(_, _))\n","      M = M_locaux.map(m => m.map(_ / numPartitions)).reduce(sum(_, _))\n","    }\n","    W\n","  }\n","\n","def MOM_MBGD_parallel_DF(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                         df: DataFrame, tailleBatch: Int, beta: Double): Array[Double] = {\n","  var W = w\n","  var M = w\n","  for (i <- 1 to epochs) {\n","    val spark = getSS\n","    import spark.implicits._\n","    val W_b = sc.broadcast(W)\n","    val M_b = sc.broadcast(M)\n","    val W_M_locaux = df.mapPartitions(\n","      iterator => { // Iterator[Row]\n","        var result = MOM_MBGD_local(W_b.value, nu, epochsLocal,\n","          arrayOfRowToArray(iterator.toIterable.toArray[Row]), tailleBatch, M_b.value, beta)\n","        Seq(result).toIterator\n","      })\n","    W = W_M_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","      .map(_.toString.toDouble / W_M_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","\n","    M = W_M_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](1).toArray[Double]\n","      .map(_.toString.toDouble / W_M_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","  }\n","  W\n","}\n","\n","def MOM_MBGD_parallel_DS(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                           ds: Dataset[GenericDSRow], tailleBatch: Int, beta: Double): Array[Double] = {\n","    var W = w\n","    var M = w\n","    for (i <- 1 to epochs) {\n","      val spark = getSS\n","      import spark.implicits._\n","      val W_b = sc.broadcast(W)\n","      val M_b = sc.broadcast(M)\n","      val W_M_locaux = ds.mapPartitions(\n","        iterator => { // Iterator[Row]\n","          val r2 = toArrayDeRow(iterator.toIterable.toArray[GenericDSRow])\n","          var result = MOM_MBGD_local(W_b.value, nu, epochsLocal,\n","            arrayOfRowToArray(r2), tailleBatch, M_b.value, beta)\n","          Seq(result).toIterator\n","        })\n","      W = W_M_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","        .map(_.toString.toDouble / W_M_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","\n","      M = W_M_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](1).toArray[Double]\n","        .map(_.toString.toDouble / W_M_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","    }\n","    W\n","  }"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">MOM_MBGD_local: (W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[(Double, Array[Double])], tailleBatch: Int, M_b: Array[Double], beta: Double)(Array[Double], Array[Double])\n","MOM_MBGD_parallel_RDD: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, r: org.apache.spark.rdd.RDD[(Double, Array[Double])], tailleBatch: Int, beta: Double)Array[Double]\n","MOM_MBGD_parallel_DF: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, df: org.apache.spark.sql.DataFrame, tailleBatch: Int, beta: Double)Array[Double]\n","MOM_MBGD_parallel_DS: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, ds: org.apache.spark.sql.Dataset[GenericDSRow], tailleBatch: Int, beta: Double)Array[Double]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"5Yir4UxzyvcC","colab_type":"code","colab":{},"outputId":"229e1b03-be62-46df-ce4a-395da23de68d"},"source":["%scala\n","\n","def ADA_MBGD_local(W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[(Double, Array[Double])], tailleBatch: Int, S_b: Array[Double], eps: Double): (Array[Double], Array[Double]) = {\n","\n","    val N = r.length\n","    var W_local = W_b\n","    var S_local = S_b\n","    for (i <- 1 to epochsLocal) {\n","      var random_idx_list = shuffle(0 to N - 1)\n","      val batches_idx = random_idx_list.grouped(tailleBatch).toList\n","      for (batch_idx <- batches_idx) {\n","        val batch = batch_idx.map(index => r.apply(index))\n","        val temp = batch.map(x => grad(x._1, W_local, x._2)).reduce(sum(_, _)).map(x => x / batch.length)\n","        val sq = mul(temp, temp)\n","        S_local = sum(S_local, sq)\n","        val nu_w = prod_by_scal(temp, nu)\n","        val root1 = root(S_local).map(x => x + eps)\n","        val div1 = div(nu_w, root1)\n","        W_local = subtr(W_local, div1)\n","      }\n","    }\n","    (W_local, S_local)\n","  }\n","\n","def ADA_MBGD_parallel_RDD(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                            r: RDD[(Double, Array[Double])], tailleBatch: Int, eps: Double): Array[Double] = {\n","\n","    val numPartitions = r.getNumPartitions\n","    //println(\"Nombre de partitions : \", numPartitions)\n","    val parts = r.glom()\n","    var W = w\n","    var S = w\n","    for (i <- 1 to epochs) {\n","      val W_b = sc.broadcast(W)\n","      val S_b = sc.broadcast(S)\n","      val W_S_locaux = parts.map(x => ADA_MBGD_local(W_b.value, nu, epochsLocal, x, tailleBatch, S_b.value, eps))\n","      val W_locaux = W_S_locaux.map(x => x._1)\n","      val S_locaux = W_S_locaux.map(x => x._2)\n","      W = W_locaux.map(w => w.map(_ / numPartitions)).reduce(sum(_, _))\n","      S = S_locaux.map(m => m.map(_ / numPartitions)).reduce(sum(_, _))\n","    }\n","    W\n"," }\n","def ADA_MBGD_parallel_DF(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                           df: DataFrame, tailleBatch: Int, eps: Double): Array[Double] = {\n","    var W = w\n","    var S = w\n","    for (i <- 1 to epochs) {\n","      val spark = getSS\n","      import spark.implicits._\n","      val W_b = sc.broadcast(W)\n","      val S_b = sc.broadcast(S)\n","      val W_S_locaux = df.mapPartitions(\n","        iterator => { // Iterator[Row]\n","          var result = ADA_MBGD_local(W_b.value, nu, epochsLocal, arrayOfRowToArray(iterator.toIterable.toArray[Row]), tailleBatch, S_b.value, eps)\n","          Seq(result).toIterator\n","        })\n","      W = W_S_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","        .map(_.toString.toDouble / W_S_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","\n","      S = W_S_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](1).toArray[Double]\n","        .map(_.toString.toDouble / W_S_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","    }\n","    W\n","  }\n","\n","def ADA_MBGD_parallel_DS(w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int,\n","                           ds: Dataset[GenericDSRow], tailleBatch: Int, eps: Double): Array[Double] = {\n","    val spark = getSS\n","    var W = w\n","    var S = w\n","    for (i <- 1 to epochs) {\n","      import spark.implicits._\n","      val W_b = sc.broadcast(W)\n","      val S_b = sc.broadcast(S)\n","      val W_S_locaux = ds.mapPartitions(\n","        iterator => { // Iterator[Row]\n","          val r2 = toArrayDeRow(iterator.toIterable.toArray[GenericDSRow])\n","          var result = ADA_MBGD_local(W_b.value, nu, epochsLocal, arrayOfRowToArray(r2), tailleBatch, S_b.value, eps)\n","          Seq(result).toIterator\n","        })\n","      W = W_S_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](0).toArray[Double]\n","        .map(_.toString.toDouble / W_S_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","\n","      S = W_S_locaux.select(\"*\").collect().map(w => w.getAs[Seq[Double]](1).toArray[Double]\n","        .map(_.toString.toDouble / W_S_locaux.rdd.getNumPartitions)).reduce(sum(_, _))\n","    } // Fin for\n","    W\n","  }"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">ADA_MBGD_local: (W_b: Array[Double], nu: Double, epochsLocal: Int, r: Array[(Double, Array[Double])], tailleBatch: Int, S_b: Array[Double], eps: Double)(Array[Double], Array[Double])\n","ADA_MBGD_parallel_RDD: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, r: org.apache.spark.rdd.RDD[(Double, Array[Double])], tailleBatch: Int, eps: Double)Array[Double]\n","ADA_MBGD_parallel_DF: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, df: org.apache.spark.sql.DataFrame, tailleBatch: Int, eps: Double)Array[Double]\n","ADA_MBGD_parallel_DS: (w: Array[Double], nu: Double, epochs: Int, epochsLocal: Int, ds: org.apache.spark.sql.Dataset[GenericDSRow], tailleBatch: Int, eps: Double)Array[Double]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Q7fFU0zlyvcF","colab_type":"code","colab":{},"outputId":"57f91cf8-f568-486e-9230-34aefe13f68d"},"source":["%scala\n","\n","def preprocess(pathFile: String, label: String, repartition: Int = 0): (DataFrame, DataFrame) = {\n","\n","    val spark = getSS\n","    import spark.implicits._\n","\n","    // Load CSV into DataFrame\n","    val df = getSS.read.format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .option(\"inferSchema\", \"true\")\n","      //.option(\"numPartitions\", partitionNumber)\n","      .load(pathFile)\n","\n","    // Repartitonment\n","    //val dfRepart = df.coalesce(repartitionNumber)\n","\n","    // Drop useless columns\n","    val dfDrop = df.drop(\"date\", \"_c0\")\n","\n","    //convert to Double\n","    //valueStr = valueStr.replace(',', '.');\n","\n","    // Add Bias column\n","    val dfWithBias = dfDrop.withColumn(\"bias\", lit(1.0))\n","    dfWithBias.show()\n","\n","    // Fill null values with mean\n","    val imputer = new Imputer()\n","      .setInputCols(dfWithBias.columns)\n","      .setOutputCols(dfWithBias.columns.map(c => s\"${c}\"))\n","      .setStrategy(\"mean\")\n","\n","    val dfCleaned = imputer.fit(dfWithBias).transform(dfWithBias)\n","\n","    // Split df into train and test set\n","    def train_test_split(data: DataFrame) = {\n","\n","      val assembler = new VectorAssembler().\n","        setInputCols(data.drop(label).columns).\n","        setOutputCol(\"features\")\n","\n","      val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 0)\n","      (assembler.transform(train), assembler.transform(test))\n","    }\n","\n","    val (train, test) = train_test_split(dfCleaned)\n","\n","\n","    // Normalize each feature to have unit standard deviation.\n","    val scaler = new StandardScaler()\n","      .setInputCol(\"features\")\n","      .setOutputCol(\"scaledFeatures\")\n","      .setWithStd(true)\n","      .setWithMean(false)\n","\n","    val scaledTrain = scaler.fit(train).transform(train)\n","    val scaledTest = scaler.fit(train).transform(test)\n","\n","    val toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\n","      .map(x => BigDecimal(x)\n","        .setScale(4, BigDecimal.RoundingMode.HALF_UP).toDouble)\n","    val toArrUdf = udf(toArr)\n","\n","    val scaledTrain_arr = scaledTrain.withColumn(\"features_arr\", toArrUdf('scaledFeatures))\n","    val scaledTest_arr = scaledTest.withColumn(\"features_arr\", toArrUdf('scaledFeatures))\n","\n","    val df_train = scaledTrain_arr.select(label, \"features_arr\").withColumnRenamed(label, \"labels\").withColumnRenamed(\"features_arr\", \"features\")\n","    val df_test = scaledTest_arr.select(label, \"features_arr\").withColumnRenamed(label, \"labels\").withColumnRenamed(\"features_arr\", \"features\")\n","\n","    if (repartition != 0) {\n","      val repart_df_train = df_train.coalesce(repartition)\n","      val repart_df_test = df_test.coalesce(repartition)\n","      return (repart_df_train, repart_df_test)\n","    }\n","\n","    (df_train, df_test)\n","  }\n","\n","def preProcessEtExportCSV() = {\n","    val spark = getSS\n","    import spark.implicits._\n","\n","    val schema = StructType(\n","      StructField(\"labels\", StringType, true) ::\n","        StructField(\"features\", ArrayType(DoubleType), false) :: Nil)\n","\n","    val (df_train_, df_test_) =\n","      preprocess(\"/FileStore/tables/mining_dataset.csv\", \"% Silica Concentrate\", 18)\n","\n","    val df_train = df_train_.toDF(\"labels\", \"features\")\n","    val df_test = df_test_.toDF(\"labels\", \"features\")\n","    val dfTrainPourExport = df_train.select($\"labels\", $\"features\"(0), $\"features\"(1), $\"features\"(2), $\"features\"(3), $\"features\"(4),\n","      $\"features\"(5), $\"features\"(6), $\"features\"(7), $\"features\"(8), $\"features\"(9),\n","      $\"features\"(10), $\"features\"(11), $\"features\"(12), $\"features\"(13), $\"features\"(14),\n","      $\"features\"(15), $\"features\"(16), $\"features\"(17), $\"features\"(18), $\"features\"(19),\n","      $\"features\"(20), $\"features\"(21), $\"features\"(22)\n","    )\n","    val dfTestPourExport = df_test.select($\"labels\", $\"features\"(0), $\"features\"(1), $\"features\"(2), $\"features\"(3), $\"features\"(4),\n","      $\"features\"(5), $\"features\"(6), $\"features\"(7), $\"features\"(8), $\"features\"(9),\n","      $\"features\"(10), $\"features\"(11), $\"features\"(12), $\"features\"(13), $\"features\"(14),\n","      $\"features\"(15), $\"features\"(16), $\"features\"(17), $\"features\"(18), $\"features\"(19),\n","      $\"features\"(20), $\"features\"(21), $\"features\"(22)\n","    )\n","\n","    exportToCSV(dfTrainPourExport, \"dtrain.csv\")\n","    exportToCSV(dfTestPourExport, \"dtest.csv\")\n","  }\n","\n","  def exportToCSV(df: DataFrame, path: String) = {\n","    val spark = getSS\n","    df.coalesce(1).write.option(\"header\", \"true\").csv(path)\n","  }\n","\n","def valeursNulles(df_train: DataFrame) = {\n","  // affiche le total de valeurs nulles par colonne\n","  import org.apache.spark.sql.functions._\n","  df_train.select(df_train.columns.map(colName => {\n","    count(when(col(colName).isNull, true)) as s\"${colName}_nulls_count\"\n","  }): _*).show(10)\n","}\n","\n","// Faire des predictions en utilisant les W issus d'une Gradient Descent\n","def predict(df_test: DataFrame, w_gd: Array[Double]): DataFrame = {\n","  val spark = getSS\n","  import spark.implicits._\n","  val pred = df_test.select(\"features\").map(row => prod_scal(row.getSeq(0).toArray, w_gd)).toDF(\"predictions\")\n","  return pred\n","}\n","\n","// R2-Score des predictions de la gradient descente (problème linéaire)\n","def r2_score(pred: DataFrame, df_test: DataFrame): Double = {\n","  val spark = getSS\n","  import spark.implicits._\n","  val y_mean = df_test.select(mean(df_test(\"labels\"))).collect()(0).getDouble(0)\n","  //println(\"y_mean\", y_mean)\n","  //val ss_res = (pred.rdd zip df_test.select(\"labels\").rdd).map(x => (x._1 - x._2.getDouble(0))).map(x => x * x).reduce(_+_)/n\n","  val ss_res = (df_test.select(\"labels\").rdd zip pred.select(\"predictions\").rdd).map(x => (x._1.getDouble(0) - x._2.getDouble(0)) * (x._1.getDouble(0) - x._2.getDouble(0))).reduce(_ + _)\n","  //println(\"res\", ss_res)\n","  val ss_tot = df_test.select(\"labels\").map(yi => (yi.getDouble(0) - y_mean) * (yi.getDouble(0) - y_mean)).reduce(_ + _)\n","  //println(\"tot\", ss_tot)\n","  val score = 1 - ss_res / ss_tot\n","  return score\n","}\n","\n","// Mean Squarred Error des predictions de la gradient descent\n","def mse(pred: DataFrame, df_test: DataFrame): Double = {\n","  val y_mean = df_test.select(mean(df_test(\"labels\"))).collect()(0).getDouble(0)\n","  println(\"y_mean\", y_mean)\n","  //val ss_res = (pred.rdd zip df_test.select(\"labels\").rdd).map(x => (x._1 - x._2.getDouble(0))).map(x => x * x).reduce(_+_)/n\n","  val mse = (df_test.select(\"labels\").rdd zip pred.select(\"predictions\").rdd).map(x => (x._1.getDouble(0) - x._2.getDouble(0)) * (x._1.getDouble(0) - x._2.getDouble(0))).reduce(_ + _) / df_test.count()\n","  return mse\n","}\n","\n","def recupCsvEnDataFrame(path: String): DataFrame = {\n","    val spark = getSS\n","    import spark.implicits._\n","    val df_ = getSS.read.format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .option(\"inferSchema\", \"true\")\n","      //.option(\"numPartitions\", partitionNumber)\n","      .load(path)\n","    val assembler = new VectorAssembler().\n","      setInputCols(df_.drop(\"labels\").columns).\n","      setOutputCol(\"features_\")\n","    val df__ = assembler.transform(df_)\n","    val df = df__.select(\"labels\", \"features_\")\n","\n","    val toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\n","      .map(x => BigDecimal(x)\n","        .setScale(4, BigDecimal.RoundingMode.HALF_UP).toDouble)\n","    val toArrUdf = udf(toArr)\n","\n","    val df2 = df.withColumn(\"features\", toArrUdf('features_))\n","    val df3 = df2.drop(\"features_\")\n","    df3\n","}"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">preprocess: (pathFile: String, label: String, repartition: Int)(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n","preProcessEtExportCSV: ()Unit\n","exportToCSV: (df: org.apache.spark.sql.DataFrame, path: String)Unit\n","valeursNulles: (df_train: org.apache.spark.sql.DataFrame)Unit\n","predict: (df_test: org.apache.spark.sql.DataFrame, w_gd: Array[Double])org.apache.spark.sql.DataFrame\n","r2_score: (pred: org.apache.spark.sql.DataFrame, df_test: org.apache.spark.sql.DataFrame)Double\n","mse: (pred: org.apache.spark.sql.DataFrame, df_test: org.apache.spark.sql.DataFrame)Double\n","recupCsvEnDataFrame: (path: String)org.apache.spark.sql.DataFrame\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"mPy09LeOyvcJ","colab_type":"code","colab":{},"outputId":"50183f72-fa5e-44d3-ef94-cc7f11a06ce3"},"source":["%scala\n","\n","def generateRDD(taille: Int, repartition: Int = 0): RDD[(Double, Array[Double])] = {\n","\n","  val r = scala.util.Random\n","  val spark = getSS\n","  import spark.implicits._\n","\n","  val f1 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f2 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f3 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val features = (f1 zip f2 zip f3).map(x => (x._1._1, x._1._2, x._2))\n","  val data = features.map(x => (5 * x._1 + 14 * x._2 + 0.5 * x._3 + 2, Array(x._1, x._2, x._3, 1)))\n","\n","\n","  if (repartition != 0) {\n","    val rdd = sc.parallelize(data, repartition)\n","    return rdd\n","  }\n","  else {\n","    val rdd = sc.parallelize(data)\n","    return rdd\n","  }\n","}\n","\n","def generateDataFrame(taille: Int, repartition: Int = 0): DataFrame = {\n","  val r = scala.util.Random\n","  val spark = getSS\n","  import spark.implicits._\n","\n","  val f1 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f2 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f3 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val features = (f1 zip f2 zip f3).map(x => (x._1._1, x._1._2, x._2))\n","  val data = features.map(x => (5 * x._1 + 14 * x._2 + 0.5 * x._3 + 2, Array(x._1, x._2, x._3, 1)))\n","\n","  if (repartition != 0) {\n","    val rdd = sc.parallelize(data, repartition)\n","    val df = rdd.toDF(\"labels\", \"features\")\n","    return df\n","  }\n","  else {\n","    val rdd = sc.parallelize(data)\n","    val df = rdd.toDF(\"labels\", \"features\")\n","    return df\n","  }\n","\n","}\n","\n","def generateDataSet(taille: Int, repartition: Int = 0): Dataset[GenericDSRow] = {\n","  val r = scala.util.Random\n","  val spark = getSS\n","  import spark.implicits._\n","\n","  val f1 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f2 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val f3 = for (i <- 1 to taille) yield r.nextInt(100).toDouble\n","  val features = (f1 zip f2 zip f3).map(x => (x._1._1, x._1._2, x._2))\n","  val data = features.map(x => (5 * x._1 + 14 * x._2 + 0.5 * x._3 + 2, Array(x._1, x._2, x._3, 1)))\n","\n","  if (repartition != 0) {\n","    val rdd = sc.parallelize(data, repartition)\n","    val df = rdd.toDF(\"labels\", \"features\")\n","    val ds: Dataset[GenericDSRow] = df.as[GenericDSRow]\n","    return ds\n","  }\n","  else {\n","    val rdd = sc.parallelize(data)\n","    val df = rdd.toDF(\"labels\", \"features\")\n","    val ds: Dataset[GenericDSRow] = df.as[GenericDSRow]\n","    return ds\n","  }\n","\n","}"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">generateRDD: (taille: Int, repartition: Int)org.apache.spark.rdd.RDD[(Double, Array[Double])]\n","generateDataFrame: (taille: Int, repartition: Int)org.apache.spark.sql.DataFrame\n","generateDataSet: (taille: Int, repartition: Int)org.apache.spark.sql.Dataset[GenericDSRow]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3fpWz5YdyvcP","colab_type":"code","colab":{},"outputId":"80b9a4fd-d533-41fc-d4ad-0fbd024998e2"},"source":["%scala\n","\n","def gridSearchGeneriqueDataSetGenere(tailleMaxDatasetAGenerer: Int,\n","                                     listePartitions: Array[Int],\n","                                     listeTypeDataset: Array[Int], // RDD = 0, DF = 1, DS = 2\n","                                     listeDataSize: Array[Int],\n","                                     listeBatchSize: Array[Int],\n","                                     listeVariantesGD: Array[Int], // Standard = 0, Momentum = 1, Adagrad = 2\n","                                     listeLocalParallele: Array[Int], // 0 = local, 1 = parralèle\n","                                     listeEpochsGlobal: Array[Int],\n","                                     listeEpochsLocal: Array[Int],\n","                                     pathCSVExport : String): DataFrame = {\n","\n","  var lignesCVS = ArrayBuffer[Row]()\n","  var mapLigneCSV = Map[String, Any]()\n","\n","  for (dataSize <- listeDataSize) {\n","    println(\"###### datasize = \" + dataSize)\n","    for (partitionsNumber <- listePartitions) {\n","      println(\"#### Partitions number = \" + partitionsNumber)\n","      val rddG = generateRDD(tailleMaxDatasetAGenerer, partitionsNumber)\n","      val dfG = generateDataFrame(tailleMaxDatasetAGenerer, partitionsNumber)\n","      val dsG = generateDataSet(tailleMaxDatasetAGenerer, partitionsNumber)\n","      val datasetGArray = rddG.collect()\n","      for (epochGlobal <- listeEpochsGlobal) {\n","        println(\"## Epoch globale = \" + epochGlobal)\n","        for (epochLocal <- listeEpochsLocal) {\n","          println(\"Epoch locale = \" + epochLocal)\n","\n","\n","          for (batchSize <- listeBatchSize) {\n","            println(\"Batch size = \" + batchSize)\n","            val spark = getSS\n","            import spark.implicits._\n","            val dfT = datasetGArray.toSeq.toDF(\"labels\", \"features\")\n","\n","            var w = Array(0.0, 0.0, 0.0, 0.0)\n","            var r = Row.empty\n","\n","            for (localParallel <- listeLocalParallele) {\n","              for (varianteGD <- listeVariantesGD) {\n","\n","                if (localParallel == 0) {\n","                  if (varianteGD == 0) {\n","                    //region Standard local\n","                    // Version standard\n","                    // Local\n","\n","                    w = Array(0.0, 0.0, 0.0, 0.0)\n","\n","                    val t0 = System.nanoTime()\n","                    w = MBGD_local_SansSpark(w, 0.00001, epochLocal, datasetGArray, batchSize)\n","                    val t1 = System.nanoTime()\n","                    val predStandardLocal = predict(dfT, w)\n","                    val scoreStandardLocal = r2_score(predStandardLocal, dfT)\n","                    //println(\"Score = \" + scoreStandardLocal)\n","                    mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                      \"score\" -> scoreStandardLocal, \"batchSize\" -> batchSize,\n","                      \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Standard\",\n","                      \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                      \"typeDataset\" -> \"Array\")\n","                    mapLigneCSV += (\"tempsExecution\" -> (t1 - t0) / 1000000000.0)\n","                    r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                      mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"),\n","                      mapLigneCSV(\"batchSize\"), mapLigneCSV(\"partitionsNumber\"),\n","                      mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                      mapLigneCSV(\"tempsExecution\")))\n","                    //println(r mkString \", \")\n","                    lignesCVS += r\n","                    //endregion\n","                  }\n","                  //region Momentum local\n","                  // Version Momentum\n","                  // Local\n","                  if (varianteGD == 1) {\n","                    w = Array(0.0, 0.0, 0.0, 0.0)\n","\n","                    val M_Mom = w\n","                    val t0ML = System.nanoTime()\n","                    w = MOM_MBGD_local(w, 0.00001, epochLocal, datasetGArray, batchSize, M_Mom, 0.90)._1\n","                    val t1ML = System.nanoTime()\n","                    val predMomentumLocal = predict(dfT, w)\n","                    val scoreMomentumLocal = r2_score(predMomentumLocal, dfT)\n","                    mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                      \"score\" -> scoreMomentumLocal, \"batchSize\" -> batchSize,\n","                      \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Momentum\",\n","                      \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                      \"typeDataset\" -> \"Array\")\n","                    //println(\"Time = \" + timeStandardLocal)\n","                    mapLigneCSV += (\"tempsExecution\" -> (t1ML - t0ML) / 1000000000.0)\n","                    r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                      mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                      mapLigneCSV(\"partitionsNumber\"),\n","                      mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                      mapLigneCSV(\"tempsExecution\")))\n","                    //println(r mkString \", \")\n","                    lignesCVS += r\n","                    //endregion\n","\n","                  }\n","                  //region Adagrad local\n","                  // Version Adagrad\n","                  // Local\n","                  if (varianteGD == 2) {\n","                    w = Array(0.0, 0.0, 0.0, 0.0)\n","\n","                    val M_Ada = w\n","                    val t0AL = System.nanoTime()\n","                    w = ADA_MBGD_local(w, 0.00001, epochLocal, datasetGArray, batchSize, M_Ada, 0.90)._1\n","                    val t1AL = System.nanoTime()\n","                    val predAdagradLocal = predict(dfT, w)\n","                    val scoreAdagradLocal = r2_score(predAdagradLocal, dfT)\n","                    //println(\"Score = \" + scoreStandardLocal)\n","                    mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                      \"score\" -> scoreAdagradLocal, \"batchSize\" -> batchSize,\n","                      \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Adagrad\",\n","                      \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                      \"typeDataset\" -> \"Array\")\n","                    mapLigneCSV += (\"tempsExecution\" -> (t1AL - t0AL) / 1000000000.0)\n","                    r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                      mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                      mapLigneCSV(\"partitionsNumber\"),\n","                      mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                      mapLigneCSV(\"tempsExecution\")))\n","                    //println(r mkString \", \")\n","                    lignesCVS += r\n","                    //endregion\n","                  }\n","                }\n","                else {\n","                  // Parallèle\n","\n","                  for (typeDS <- listeTypeDataset) {\n","                    if (varianteGD == 0) {\n","                      //<editor-fold desc=\"Parallele standard\">\n","                      // Standard\n","\n","                      if (typeDS == 0) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PS = System.nanoTime()\n","                        w = MBGD_parallel_RDD(w, 0.00001, epochGlobal, epochLocal, rddG, batchSize)\n","                        val t1PS = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                      }\n","\n","                      if (typeDS == 1) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PS = System.nanoTime()\n","                        w = MBGD_parallel_DF(w, 0.00001, epochGlobal, epochLocal, dfG, batchSize)\n","                        val t1PS = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                      }\n","                      if (typeDS == 2) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PS = System.nanoTime()\n","                        w = MBGD_parallel_DS(w, 0.00001, epochGlobal, epochLocal, dsG, batchSize)\n","                        val t1PS = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                      }\n","                      val predStandardParallele = predict(dfG, w)\n","                      val scoreStandardParallele = r2_score(predStandardParallele, dfG)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreStandardParallele, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Standard\",\n","                        \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                        mapLigneCSV(\"partitionsNumber\"),\n","                        mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                        mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //</editor-fold>\n","                    }\n","                    //region Paralle Momentum\n","                    // Momentum\n","                    if (varianteGD == 1) {\n","                      if (typeDS == 0) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PM = System.nanoTime()\n","                        w = MOM_MBGD_parallel_RDD(w, 0.00001, epochGlobal, epochLocal,\n","                          rddG, batchSize, 0.90)\n","                        val t1PM = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                      }\n","\n","                      if (typeDS == 1) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PM = System.nanoTime()\n","                        w = MOM_MBGD_parallel_DF(w, 0.00001, epochGlobal, epochLocal,\n","                          dfG, batchSize, 0.90)\n","                        val t1PM = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                      }\n","                      if (typeDS == 2) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PM = System.nanoTime()\n","                        w = MOM_MBGD_parallel_DS(w, 0.00001, epochGlobal, epochLocal,\n","                          dsG, batchSize, 0.90)\n","                        val t1PM = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                      }\n","                      val predParalleleMomentum = predict(dfG, w)\n","                      val scoreParalleleMomentum = r2_score(predParalleleMomentum, dfG)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreParalleleMomentum, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Momentum\",\n","                        \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"),\n","                        mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"), mapLigneCSV(\"partitionsNumber\"),\n","                        mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                        mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //endregion\n","                    }\n","                    if (varianteGD == 2) {\n","                      //region Parallele Adagrad\n","                      // Adagrad\n","                      if (typeDS == 0) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PA = System.nanoTime()\n","                        w = ADA_MBGD_parallel_RDD(w, 10, epochGlobal, epochLocal,\n","                          rddG, batchSize, 0.000000001)\n","                        val t1PA = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                      }\n","                      if (typeDS == 1) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PA = System.nanoTime()\n","                        w = ADA_MBGD_parallel_DF(w, 10, epochGlobal, epochLocal,\n","                          dfG, batchSize, 0.000000001)\n","                        val t1PA = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                      }\n","                      if (typeDS == 2) {\n","                        w = Array(0.0, 0.0, 0.0, 0.0)\n","                        val t0PA = System.nanoTime()\n","                        w = ADA_MBGD_parallel_DS(w, 10, epochGlobal, epochLocal,\n","                          dsG, batchSize, 0.000000001)\n","                        val t1PA = System.nanoTime()\n","                        mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                        mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                      }\n","                      val predParalleleAdagrad = predict(dfG, w)\n","                      val scoreParalleleAdagrad = r2_score(predParalleleAdagrad, dfG)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreParalleleAdagrad, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Adagrad\",\n","                        \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                        mapLigneCSV(\"partitionsNumber\"),\n","                        mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                        mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //endregion\n","                    }\n","                  }\n","                }\n","              }\n","            } // localParallele\n","          } // batchSize\n","        } // epochLocal\n","      } // epochGlobal\n","    } // partitionNumber\n","  } // dataSize\n","\n","  val rdd = sc.makeRDD(lignesCVS)\n","  val schema = StructType(\n","    StructField(\"varianteGD\", StringType, false) ::\n","      StructField(\"typeDataset\", StringType, false) ::\n","      StructField(\"localParallele\", StringType, false) ::\n","      StructField(\"dataSize\", IntegerType, false) ::\n","      StructField(\"batchSize\", IntegerType, false) ::\n","      StructField(\"partitionsNumber\", IntegerType, false) ::\n","      StructField(\"epochGlobal\", IntegerType, true) ::\n","      StructField(\"epochLocal\", IntegerType, false) ::\n","      StructField(\"score\", DoubleType, false) ::\n","      StructField(\"tempsExecutionMs\", DoubleType, false) ::\n","      Nil)\n","  val df = getSS.createDataFrame(rdd, schema)\n","  df.show(false)\n","  exportToCSV(df, pathCSVExport)\n","  df\n","\n","}"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">gridSearchGeneriqueDataSetGenere: (tailleMaxDatasetAGenerer: Int, listePartitions: Array[Int], listeTypeDataset: Array[Int], listeDataSize: Array[Int], listeBatchSize: Array[Int], listeVariantesGD: Array[Int], listeLocalParallele: Array[Int], listeEpochsGlobal: Array[Int], listeEpochsLocal: Array[Int], pathCSVExport: String)org.apache.spark.sql.DataFrame\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"nMrc9dD4yvcU","colab_type":"code","colab":{},"outputId":"76a2fb32-8f9c-46c1-bc13-32bac952b30f"},"source":["%scala\n","\n","gridSearchGeneriqueDataSetGenere(\n","    listePartitions = Array(4, 8),\n","    listeBatchSize = Array(1, 32),\n","    listeDataSize = Array(10000),\n","    listeEpochsGlobal = Array(1),\n","    listeEpochsLocal = Array(1),\n","    listeLocalParallele = Array(1),\n","    listeTypeDataset = Array(0, 1, 2),\n","    listeVariantesGD = Array(0, 1, 2),\n","    tailleMaxDatasetAGenerer = 10000,\n","    pathCSVExport = \"grid_DSG.csv\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">###### datasize = 10000\n","#### Partitions number = 4\n","## Epoch globale = 1\n","Epoch locale = 1\n","Batch size = 1\n","Batch size = 32\n","#### Partitions number = 8\n","## Epoch globale = 1\n","Epoch locale = 1\n","Batch size = 1\n","Batch size = 32\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+------------------+----------------+\n","varianteGD|typeDataset|localParallele|dataSize|batchSize|partitionsNumber|epochGlobal|epochLocal|score             |tempsExecutionMs|\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+------------------+----------------+\n","Standard  |RDD        |Parallele     |10000   |1        |4               |1          |1         |0.999998008962841 |0.120579313     |\n","Standard  |DataFrame  |Parallele     |10000   |1        |4               |1          |1         |0.9999980076745929|0.397244989     |\n","Standard  |Dataset    |Parallele     |10000   |1        |4               |1          |1         |0.9999980324497575|0.569172605     |\n","Momentum  |RDD        |Parallele     |10000   |1        |4               |1          |1         |0.9999979260908611|0.27077085      |\n","Momentum  |DataFrame  |Parallele     |10000   |1        |4               |1          |1         |0.999996819734835 |0.466785065     |\n","Momentum  |Dataset    |Parallele     |10000   |1        |4               |1          |1         |0.9999974859555045|1.370326248     |\n","Adagrad   |RDD        |Parallele     |10000   |1        |4               |1          |1         |0.9999992609339164|0.32436919      |\n","Adagrad   |DataFrame  |Parallele     |10000   |1        |4               |1          |1         |0.9999980512356682|0.5497383       |\n","Adagrad   |Dataset    |Parallele     |10000   |1        |4               |1          |1         |0.9999981883574389|0.588356174     |\n","Standard  |RDD        |Parallele     |10000   |32       |4               |1          |1         |0.9679128557295619|0.135108318     |\n","Standard  |DataFrame  |Parallele     |10000   |32       |4               |1          |1         |0.9710349921127938|0.403139406     |\n","Standard  |Dataset    |Parallele     |10000   |32       |4               |1          |1         |0.9715159845029212|0.531396162     |\n","Momentum  |RDD        |Parallele     |10000   |32       |4               |1          |1         |0.9999076741355732|0.247969622     |\n","Momentum  |DataFrame  |Parallele     |10000   |32       |4               |1          |1         |0.9999138024424494|0.433576663     |\n","Momentum  |Dataset    |Parallele     |10000   |32       |4               |1          |1         |0.9998232521184419|0.516747161     |\n","Adagrad   |RDD        |Parallele     |10000   |32       |4               |1          |1         |0.9999888559427584|0.331804937     |\n","Adagrad   |DataFrame  |Parallele     |10000   |32       |4               |1          |1         |0.9999879888103629|0.581573525     |\n","Adagrad   |Dataset    |Parallele     |10000   |32       |4               |1          |1         |0.9999775729689085|0.956235085     |\n","Standard  |RDD        |Parallele     |10000   |1        |8               |1          |1         |0.9999980134886589|0.172270133     |\n","Standard  |DataFrame  |Parallele     |10000   |1        |8               |1          |1         |0.9999980725917219|0.52189817      |\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+------------------+----------------+\n","only showing top 20 rows\n","\n","res1: org.apache.spark.sql.DataFrame = [varianteGD: string, typeDataset: string ... 8 more fields]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hj72D6nMyvcX","colab_type":"code","colab":{},"outputId":"3dff54e7-60f2-4d95-ebb7-f77e736fe3df"},"source":["%scala\n","\n","def gridSearchGeneriqueMinierDataSet(pathDatasetTrain: String = \"/FileStore/tables/dtrain.csv\",\n","                                       pathDatasetTest: String = \"/FileStore/tables/dtest.csv\",\n","                                       listePartitions: Array[Int],\n","                                       listeTypeDataset: Array[Int], // RDD = 0, DF = 1, DS = 2\n","                                       listeDataSize: Array[Int],\n","                                       listeBatchSize: Array[Int],\n","                                       listeVariantesGD: Array[Int], // Standard = 0, Momentum = 1, Adagrad = 2\n","                                       listeLocalParallele: Array[Int], // 0 = local, 1 = parralèle\n","                                       listeEpochsGlobal: Array[Int],\n","                                       listeEpochsLocal: Array[Int],\n","                                       pathCVSExport : String\n","                                      ): DataFrame = {\n","\n","    var lignesCVS = ArrayBuffer[Row]()\n","    var mapLigneCSV = Map[String, Any]()\n","\n","    for (dataSize <- listeDataSize) {\n","      println(\"###### datasize = \" + dataSize)\n","      for (partitionsNumber <- listePartitions) {\n","\n","        println(\"#### Partitions number = \" + partitionsNumber)\n","\n","        val spark = getSS\n","        import spark.implicits._\n","\n","        val dfTrain: DataFrame = recupCsvEnDataFrame(pathDatasetTrain).coalesce(partitionsNumber)\n","        //display(dfTrain)\n","        val dfTest: DataFrame = recupCsvEnDataFrame(pathDatasetTest).coalesce(partitionsNumber)\n","        val dsTrain: Dataset[GenericDSRow] = dfTrain.as[GenericDSRow]\n","        val dsTest: Dataset[GenericDSRow] = dfTest.as[GenericDSRow]\n","        val rddTrain = dfTrain.rdd.map(x => (x.getDouble(0), x.getSeq(1).toArray[Double]))\n","        val rddTest = dfTest.rdd.map(x => (x.getDouble(0), x.getSeq(1).toArray[Double]))\n","        val datasetTrainArray = rddTrain.collect()\n","        val datasetTestArray = rddTest.collect()\n","\n","        for (epochGlobal <- listeEpochsGlobal) {\n","          println(\"## Epoch globale = \" + epochGlobal)\n","          for (epochLocal <- listeEpochsLocal) {\n","            println(\"Epoch locale = \" + epochLocal)\n","\n","            for (batchSize <- listeBatchSize) {\n","              println(\"Batch size = \" + batchSize)\n","\n","              val spark = getSS\n","              import spark.implicits._\n","\n","              val dfT = datasetTestArray.toSeq.toDF(\"labels\", \"features\")\n","\n","              var w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","              var r = Row.empty\n","\n","              for (localParallel <- listeLocalParallele) {\n","                for (varianteGD <- listeVariantesGD) {\n","\n","                  if (localParallel == 0) {\n","                    if (varianteGD == 0) {\n","                      //region Standard local\n","                      // Version standard\n","                      // Local\n","\n","                      w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","\n","                      val t0 = System.nanoTime()\n","                      w = MBGD_local_SansSpark(w, 0.00001, epochLocal, datasetTrainArray, batchSize)\n","                      val t1 = System.nanoTime()\n","                      val predStandardLocal = predict(dfT, w)\n","                      val scoreStandardLocal = r2_score(predStandardLocal, dfT)\n","                      //println(\"Score = \" + scoreStandardLocal)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreStandardLocal, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Standard\",\n","                        \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                        \"typeDataset\" -> \"Array\")\n","                      mapLigneCSV += (\"tempsExecution\" -> (t1 - t0) / 1000000000.0)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                        mapLigneCSV(\"partitionsNumber\"),\n","                        mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                        mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //endregion\n","                    }\n","                    //region Momentum local\n","                    // Version Momentum\n","                    // Local\n","                    if (varianteGD == 1) {\n","                      w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","\n","                      val M_Mom = w\n","                      val t0ML = System.nanoTime()\n","                      w = MOM_MBGD_local(w, 0.00001, epochLocal, datasetTrainArray,\n","                        batchSize, M_Mom, 0.90)._1\n","                      val t1ML = System.nanoTime()\n","                      val predMomentumLocal = predict(dfT, w)\n","                      val scoreMomentumLocal = r2_score(predMomentumLocal, dfT)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreMomentumLocal, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Momentum\",\n","                        \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                        \"typeDataset\" -> \"Array\")\n","                      //println(\"Time = \" + timeStandardLocal)\n","                      mapLigneCSV += (\"tempsExecution\" -> (t1ML - t0ML) / 1000000000.0)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                        mapLigneCSV(\"partitionsNumber\"), mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"),\n","                        mapLigneCSV(\"score\"), mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //endregion\n","\n","                    }\n","                    //region Adagrad local\n","                    // Version Adagrad\n","                    // Local\n","                    if (varianteGD == 2) {\n","                      w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","\n","                      val M_Ada = w\n","                      val t0AL = System.nanoTime()\n","                      w = ADA_MBGD_local(w, 0.00001, epochLocal,\n","                        datasetTrainArray, batchSize, M_Ada, 0.90)._1\n","                      val t1AL = System.nanoTime()\n","                      val predAdagradLocal = predict(dfT, w)\n","                      val scoreAdagradLocal = r2_score(predAdagradLocal, dfT)\n","                      //println(\"Score = \" + scoreStandardLocal)\n","                      mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                        \"score\" -> scoreAdagradLocal, \"batchSize\" -> batchSize,\n","                        \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Adagrad\",\n","                        \"localParallele\" -> \"Local\", \"dataSize\" -> dataSize,\n","                        \"typeDataset\" -> \"Array\")\n","                      mapLigneCSV += (\"tempsExecution\" -> (t1AL - t0AL) / 1000000000.0)\n","                      r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                        mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                        mapLigneCSV(\"partitionsNumber\"),\n","                        mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                        mapLigneCSV(\"tempsExecution\")))\n","                      //println(r mkString \", \")\n","                      lignesCVS += r\n","                      //endregion\n","                    }\n","                  }\n","                  else {\n","                    // Parallèle\n","\n","                    for (typeDS <- listeTypeDataset) {\n","                      if (varianteGD == 0) {\n","                        //<editor-fold desc=\"Parallele standard\">\n","                        // Standard\n","\n","                        if (typeDS == 0) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PS = System.nanoTime()\n","                          w = MBGD_parallel_RDD(w, 0.00001, epochGlobal, epochLocal, rddTrain, batchSize)\n","                          val t1PS = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                        }\n","\n","                        if (typeDS == 1) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PS = System.nanoTime()\n","                          w = MBGD_parallel_DF(w, 0.00001, epochGlobal, epochLocal, dfTrain, batchSize)\n","                          val t1PS = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                        }\n","                        if (typeDS == 2) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PS = System.nanoTime()\n","                          w = MBGD_parallel_DS(w, 0.00001, epochGlobal, epochLocal, dsTrain, batchSize)\n","                          val t1PS = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PS - t0PS) / 1000000000.0)\n","                        }\n","                        val predStandardParallele = predict(dfTest, w)\n","                        val scoreStandardParallele = r2_score(predStandardParallele, dfTest)\n","                        mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                          \"score\" -> scoreStandardParallele, \"batchSize\" -> batchSize,\n","                          \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Standard\",\n","                          \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                        r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                          mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                          mapLigneCSV(\"partitionsNumber\"),\n","                          mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                          mapLigneCSV(\"tempsExecution\")))\n","                        //println(r mkString \", \")\n","                        lignesCVS += r\n","                        //</editor-fold>\n","                      }\n","                      //region Paralle Momentum\n","                      // Momentum\n","                      if (varianteGD == 1) {\n","                        if (typeDS == 0) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PM = System.nanoTime()\n","                          w = MOM_MBGD_parallel_RDD(w, 0.00001, epochGlobal, epochLocal,\n","                            rddTrain, batchSize, 0.90)\n","                          val t1PM = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                        }\n","\n","                        if (typeDS == 1) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PM = System.nanoTime()\n","                          w = MOM_MBGD_parallel_DF(w, 0.00001, epochGlobal, epochLocal,\n","                            dfTrain, batchSize, 0.90)\n","                          val t1PM = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                        }\n","                        if (typeDS == 2) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PM = System.nanoTime()\n","                          w = MOM_MBGD_parallel_DS(w, 0.00001, epochGlobal, epochLocal,\n","                            dsTrain, batchSize, 0.90)\n","                          val t1PM = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PM - t0PM) / 1000000000.0)\n","                        }\n","                        val predParalleleMomentum = predict(dfTest, w)\n","                        val scoreParalleleMomentum = r2_score(predParalleleMomentum, dfTest)\n","                        mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                          \"score\" -> scoreParalleleMomentum, \"batchSize\" -> batchSize,\n","                          \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Momentum\",\n","                          \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                        r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                          mapLigneCSV(\"localParallele\"),\n","                          mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"), mapLigneCSV(\"partitionsNumber\"),\n","                          mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                          mapLigneCSV(\"tempsExecution\")))\n","                        //println(r mkString \", \")\n","                        lignesCVS += r\n","                        //endregion\n","                      }\n","                      if (varianteGD == 2) {\n","                        //region Parallele Adagrad\n","                        // Adagrad\n","                        if (typeDS == 0) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PA = System.nanoTime()\n","                          w = ADA_MBGD_parallel_RDD(w, 10, epochGlobal, epochLocal,\n","                            rddTrain, batchSize, 0.000000001)\n","                          val t1PA = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"RDD\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                        }\n","                        if (typeDS == 1) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PA = System.nanoTime()\n","                          w = ADA_MBGD_parallel_DF(w, 10, epochGlobal, epochLocal,\n","                            dfTrain, batchSize, 0.000000001)\n","                          val t1PA = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"DataFrame\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                        }\n","                        if (typeDS == 2) {\n","                          w = Array(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n","                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n","                          val t0PA = System.nanoTime()\n","                          w = ADA_MBGD_parallel_DS(w, 10, epochGlobal, epochLocal,\n","                            dsTrain, batchSize, 0.000000001)\n","                          val t1PA = System.nanoTime()\n","                          mapLigneCSV += (\"typeDataset\" -> \"Dataset\")\n","                          mapLigneCSV += (\"tempsExecution\" -> (t1PA - t0PA) / 1000000000.0)\n","                        }\n","                        val predParalleleAdagrad = predict(dfTest, w)\n","                        val scoreParalleleAdagrad = r2_score(predParalleleAdagrad, dfTest)\n","                        mapLigneCSV += (\"epochGlobal\" -> epochGlobal, \"epochLocal\" -> epochLocal,\n","                          \"score\" -> scoreParalleleAdagrad, \"batchSize\" -> batchSize,\n","                          \"partitionsNumber\" -> partitionsNumber, \"varianteGD\" -> \"Adagrad\",\n","                          \"localParallele\" -> \"Parallele\", \"dataSize\" -> dataSize)\n","                        r = Row.fromSeq(Seq(mapLigneCSV(\"varianteGD\"), mapLigneCSV(\"typeDataset\"),\n","                          mapLigneCSV(\"localParallele\"), mapLigneCSV(\"dataSize\"), mapLigneCSV(\"batchSize\"),\n","                          mapLigneCSV(\"partitionsNumber\"),\n","                          mapLigneCSV(\"epochGlobal\"), mapLigneCSV(\"epochLocal\"), mapLigneCSV(\"score\"),\n","                          mapLigneCSV(\"tempsExecution\")))\n","                        //println(r mkString \", \")\n","                        lignesCVS += r\n","                        //endregion\n","                      }\n","                    }\n","                  }\n","                }\n","              } // localParallele\n","            } // batchSize\n","          } // epochLocal\n","        } // epochGlobal\n","      } // partitionNumber\n","    } // dataSize\n","\n","    val rdd = sc.makeRDD(lignesCVS)\n","    val schema = StructType(\n","      StructField(\"varianteGD\", StringType, false) ::\n","        StructField(\"typeDataset\", StringType, false) ::\n","        StructField(\"localParallele\", StringType, false) ::\n","        StructField(\"dataSize\", IntegerType, false) ::\n","        StructField(\"batchSize\", IntegerType, false) ::\n","        StructField(\"partitionsNumber\", IntegerType, false) ::\n","        StructField(\"epochGlobal\", IntegerType, true) ::\n","        StructField(\"epochLocal\", IntegerType, false) ::\n","        StructField(\"score\", DoubleType, false) ::\n","        StructField(\"tempsExecutionMs\", DoubleType, false) ::\n","        Nil)\n","    val df = getSS.createDataFrame(rdd, schema)\n","    df.show(false)\n","    exportToCSV(df, pathCVSExport)\n","    df\n","\n","  }"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">gridSearchGeneriqueMinierDataSet: (pathDatasetTrain: String, pathDatasetTest: String, listePartitions: Array[Int], listeTypeDataset: Array[Int], listeDataSize: Array[Int], listeBatchSize: Array[Int], listeVariantesGD: Array[Int], listeLocalParallele: Array[Int], listeEpochsGlobal: Array[Int], listeEpochsLocal: Array[Int], pathCVSExport: String)org.apache.spark.sql.DataFrame\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"6bzScpysyvce","colab_type":"code","colab":{},"outputId":"1ae07979-992c-4b31-9817-3dc9b4edcbdc"},"source":["%scala\n","\n","gridSearchGeneriqueMinierDataSet(\n","  listePartitions = Array(4),\n","  listeBatchSize = Array(32),\n","  listeDataSize = Array(1000),\n","  listeEpochsGlobal = Array(1),\n","  listeEpochsLocal = Array(1),\n","  listeLocalParallele = Array(1),\n","  listeTypeDataset = Array(0, 1, 2),\n","  listeVariantesGD = Array(0, 1, 2),\n","  pathCVSExport = \"grid_Minier.csv\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">###### datasize = 1000\n","#### Partitions number = 4\n","## Epoch globale = 1\n","Epoch locale = 1\n","Batch size = 32\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+-------------------+----------------+\n","varianteGD|typeDataset|localParallele|dataSize|batchSize|partitionsNumber|epochGlobal|epochLocal|score              |tempsExecutionMs|\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+-------------------+----------------+\n","Standard  |RDD        |Parallele     |1000    |32       |4               |1          |1         |0.11222457194232505|13.281573896    |\n","Standard  |DataFrame  |Parallele     |1000    |32       |4               |1          |1         |0.11497307678722335|19.863995809    |\n","Standard  |Dataset    |Parallele     |1000    |32       |4               |1          |1         |0.08289864437545635|17.55715061     |\n","Momentum  |RDD        |Parallele     |1000    |32       |4               |1          |1         |0.49566338351499073|26.256745737    |\n","Momentum  |DataFrame  |Parallele     |1000    |32       |4               |1          |1         |0.532374057350622  |29.299834791    |\n","Momentum  |Dataset    |Parallele     |1000    |32       |4               |1          |1         |0.525926225809474  |38.057297091    |\n","Adagrad   |RDD        |Parallele     |1000    |32       |4               |1          |1         |0.22979558947779133|26.429677172    |\n","Adagrad   |DataFrame  |Parallele     |1000    |32       |4               |1          |1         |0.08548820034932625|30.03778549     |\n","Adagrad   |Dataset    |Parallele     |1000    |32       |4               |1          |1         |0.22876741100393494|41.339843809    |\n","+----------+-----------+--------------+--------+---------+----------------+-----------+----------+-------------------+----------------+\n","\n","res6: org.apache.spark.sql.DataFrame = [varianteGD: string, typeDataset: string ... 8 more fields]\n","</div>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8UAbgsGmyvci","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}