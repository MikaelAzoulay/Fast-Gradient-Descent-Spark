{"cells":[{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions.lit\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.ml.feature.Imputer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.linalg.DenseVector\nimport org.apache.spark.ml.linalg.SparseVector\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.feature.OneHotEncoderEstimator\nimport org.apache.spark.ml.feature.{Imputer, StandardScaler, StringIndexer, VectorAssembler}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\n\nval path = \"/FileStore/tables/diamonds.csv\"\nval label = \"price\"\n\n// Load CSV into DataFrame\nval dfBrut = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n//.option(\"numPartitions\", partitionNumber)\n.load(path)\n\nval df = dfBrut.select($\"carat\", $\"cut\", $\"color\", $\"clarity\", $\"depth\", $\"table\",$\"price\".cast(DoubleType).as(\"price\"), $\"x\", $\"y\", $\"z\")\n\n// Fill null values with mean\nval imputer = new Imputer()\n.setInputCols(df.drop(\"cut\", \"color\", \"clarity\").columns)\n.setOutputCols(df.drop(\"cut\", \"color\", \"clarity\").columns.map(c => s\"${c}\"))\n.setStrategy(\"mean\")\n\nval dfCleaned = imputer.fit(df).transform(df)\n\n//Indexing categorical features\nval cutIndexer = new StringIndexer().setInputCol(\"cut\").setOutputCol(\"cutIndex\")\nval cutIndexed = cutIndexer.fit(dfCleaned).transform(dfCleaned)\nval colorIndexer = new StringIndexer().setInputCol(\"color\").setOutputCol(\"colorIndex\")\nval colorIndexed = colorIndexer.fit(cutIndexed).transform(cutIndexed)\nval clarityIndexer = new StringIndexer().setInputCol(\"clarity\").setOutputCol(\"clarityIndex\")\nval clarityIndexed = clarityIndexer.fit(colorIndexed).transform(colorIndexed)\n\n// One hot encoding indexed categorical features\nval encoder = new OneHotEncoderEstimator()\n.setInputCols(Array(\"cutIndex\", \"colorIndex\", \"clarityIndex\"))\n.setOutputCols(Array(\"cutVec\", \"colorVec\", \"clarityVec\"))\nval modelEncoder = encoder.fit(clarityIndexed)\nval encoded = modelEncoder.transform(clarityIndexed).cache()\n\n// Assembling features\nval assembler = new VectorAssembler().\nsetInputCols(Array(\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\", \"cutVec\", \"colorVec\", \"clarityVec\")).\nsetOutputCol(\"features\")\n\nval output = assembler.transform(encoded)\n\n\n// Normalize each feature to have unit standard deviation.\nval scaler = new StandardScaler()\n.setInputCol(\"features\")\n.setOutputCol(\"scaledFeatures\")\n.setWithStd(true)\n.setWithMean(false)\n\nval scaledOutput = scaler.fit(output).transform(output)\n\n\n// Sparse to array conversion for compatibility with our GD\nval toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\nval toArrUdf = udf(toArr)\nval asDense = udf((v: SparseVector) => v.toDense)\n\nval output_dense = scaledOutput.withColumn(\"features_dense\", asDense($\"scaledFeatures\"))\n\nval output_arr = output_dense.withColumn(\"features_arr\", toArrUdf('features_dense))\n\n// These df are the same but compatible with our GD\n// val train_arr = train.select(label, \"features_arr\").withColumnRenamed(label, \"labels\").withColumnRenamed(\"features_arr\", \"features\")\n// val test_arr = test.select(label, \"features_arr\").withColumnRenamed(label, \"labels\").withColumnRenamed(\"features_arr\", \"features\")\n\n\n// PIPELINE\n\nval steps: Array[org.apache.spark.ml.PipelineStage] = Array(imputer, cutIndexer, colorIndexer, clarityIndexer, encoder, assembler, scaler)\n\nval pipeline_prep = new Pipeline().setStages(steps)\n\nval outputPipe = (pipeline_prep.fit(df)).transform(df)\n\n// outputPipe.show(truncate=false)\n\n// Split df into train and test set\nval Array(train, test) = outputPipe.randomSplit(Array(0.8, 0.2), seed = 0)\n\nval lr = new LinearRegression().setLabelCol(label).setFeaturesCol(\"features\")\n\nval model = lr.fit(train)\n\nval holdout = model.transform(test)\n\n\nval r2Evaluator = new RegressionEvaluator().setLabelCol(\"price\").setMetricName(\"r2\")\n\nprintln(\"r2 = \" + r2Evaluator.evaluate(holdout))\n\n/*\n\ndef diamonds_gridsearch_fast_sgd_perf(df_train: DataFrame, df_test: DataFrame, epochsLocal : Array[Int], epochsGlobal : Int): DataFrame ={\n    val w = Array(0.0, 0.0,0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n \n\n    var grid = List[Array[Double]]()\n    for (epoch <- epochsLocal){\n        // MBGD\n        val t0_mbgd = System.nanoTime()\n        val w_mbgd = MBGD_parallel(w, 0.00001, epochsGlobal, epoch, df_train, 1)\n        val t1_mbgd = System.nanoTime()\n        val pred_mbgd = predict(df_test, w_mbgd)\n        val error_mbgd = mse(pred_mbgd, df_test)\n        val score_mbgd = r2_score(pred_mbgd, df_test)\n        \n        // MBGD with Momentum 0.9\n        val t0_mom = System.nanoTime()\n        val w_mom = MOM_MBGD_parallel(w, 0.00001, epochsGlobal, epoch, df_train, 1, 0.9)\n        val t1_mom = System.nanoTime()\n        val pred_mom = predict(df_test, w_mom)\n        val score_mom = r2_score(pred_mom, df_test)\n        val error_mom = mse(pred_mom, df_test)\n\n \n\n        // MBGD with Adagrad\n        val t0_ada = System.nanoTime()\n        val w_ada = ADA_MBGD_parallel(w, 10, epochsGlobal, epoch, df_train, 1, 0.000000001)\n        val t1_ada = System.nanoTime()\n        val pred_ada = predict(df_test, w_ada)\n        val score_ada = r2_score(pred_ada, df_test)\n        val error_ada = mse(pred_ada, df_test)\n        \n        val perf = Array[Double]((epoch*epochsGlobal).toDouble, score_mbgd, error_mbgd, (t1_mbgd-t0_mbgd)/1000000000.0, score_mom, error_mom, (t1_mom-t0_mom)/1000000000.0, score_ada, error_ada, (t1_ada-t0_ada)/1000000000.0)\n        grid = perf :: grid \n     }\n    val arr = grid.toArray\n    val grid_df = sc.parallelize(arr).map(x => (x(0), x(1), x(2), x(3), x(4), x(5), x(6), x(7), x(8), x(9))).toDF(\"epoch\", \"mbgd score\", \"mbgd error\", \"mbgd time\", \"mom score\", \"mom error\", \"mom time\", \"ada score\",\"ada error\", \"ada time\")\n    return grid_df\n}\n\n*/\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">r2 = 0.9225375318106301\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions.lit\nimport org.apache.spark.sql.types._\nimport org.apache.spark.ml.feature.Imputer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.linalg.DenseVector\nimport org.apache.spark.ml.linalg.SparseVector\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.feature.OneHotEncoderEstimator\nimport org.apache.spark.ml.feature.{Imputer, StandardScaler, StringIndexer, VectorAssembler}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\npath: String = /FileStore/tables/diamonds.csv\nlabel: String = price\ndfBrut: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 8 more fields]\ndf: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 8 more fields]\nimputer: org.apache.spark.ml.feature.Imputer = imputer_030aa128939b\ndfCleaned: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 8 more fields]\ncutIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_6aca3036f5ea\ncutIndexed: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 9 more fields]\ncolorIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_454417e8e6b0\ncolorIndexed: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 10 more fields]\nclarityIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_6d91ecfc5d31\nclarityIndexed: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 11 more fields]\nencoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_d36e38a1f564\nmodelEncoder: org.apache.spark.ml.feature.OneHotEncoderModel = oneHotEncoder_d36e38a1f564\nencoded: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carat: double, cut: string ... 14 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ce39d047b6e7\noutput: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 15 more fields]\nscaler: org.apache.spark.ml.feature.StandardScaler = stdScal_38a9002f02ed\nscaledOutput: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 16 more fields]\ntoArr: Any =&gt; Array[Double] = &lt;function1&gt;\ntoArrUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,ArrayType(DoubleType,false),None)\nasDense: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\noutput_dense: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 17 more fields]\noutput_arr: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 18 more fields]\nsteps: Array[org.apache.spark.ml.PipelineStage] = Array(imputer_030aa128939b, strIdx_6aca3036f5ea, strIdx_454417e8e6b0, strIdx_6d91ecfc5d31, oneHotEncoder_d36e38a1f564, vecAssembler_ce39d047b6e7, stdScal_38a9002f02ed)\npipeline_prep: org.apache.spark.ml.Pipeline = pipeline_e1060fb9fe9a\noutputPipe: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 16 more fields]\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carat: double, cut: string ... 16 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [carat: double, cut: string ... 16 more fields]\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_6832d76c85d6\nmodel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_6832d76c85d6\nholdout: org.apache.spark.sql.DataFrame = [carat: double, cut: string ... 17 more fields]\nr2Evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_9fdd0751d820\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"DiamondsLab_NB","notebookId":3487412734444859},"nbformat":4,"nbformat_minor":0}
